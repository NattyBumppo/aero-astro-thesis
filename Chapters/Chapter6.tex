
\chapter{Analytical and Visualization Improvements in Response to Intermediate Test Data}

This chapter discusses some of the improvements we made in response to problems identified in usability and field testing.

\section{Dimensional Reduction}

In order to reduce complexity of the displayed data to minimize cognitive load, we first looked at methods for reducing the total amount of data that's displayed.

\subsection{Applying PCA to Raw Telemetry and Correlative Telemetry Models}

As we have seen before, the raw, telemetered state history for system telemetry is a matrix $M \in \mathbb{R}^{n \times t}$, while the correlative state history of a system is even higher order, on the order of $C \in \mathbb{R}^{n \times n \times t}$. Reducing either of these matrices to their principal components could be immensely valuable towards simplifying their dynamics. The post-PCA result would be a new, simplified and reconstructed state vector that captures the majority of the state dynamics. It should be noted that this technique has been shown to be of value in \cite{villegas2010principal}.

We can apply PCA, as described in Chapter 3, to first simplify the non-correlative state matrix obtained from raw telemetry. We will use sample data from the usability test simulation discussed in Chapter 5. Refer to Chapter 3 for discussion of the theoretical underpinnings of PCA.

After the SVD was performed on this data, scaled singular values were obtained as shown in Fig.~\ref{fig:raw_telem_svs}. Though the state vector of the original system is 112-dimensional, the SVD shows only 29 singular values above 15\% contribution, suggesting a system of dimensionality ~29. We can use a reduced singular value matrix $\Sigma_{r}$, containing only this subset of the 29 largest singular values, to reconstruct the state dynamics of the system. Though we cannot easily plot such a reconstruction to show its faithfulness visually, we can calculate a norm difference between the standardized original and reconstructed matrices, and in doing so, we find a standardized difference of $\sim11.41$.

\begin{figure}[h]
\centering
    \includegraphics[width=\columnwidth]{images/raw_telem_svs.png}
    \caption{Singular values are shown for the Principal Component Analysis of non-correlative, sample mission telemetry data.}
    \label{fig:raw_telem_svs}
\end{figure}

A similar deconstruction and reconstruction can be applied to the correlative models to reduce dimensionality; however, by calculating new bases for analysis, an intrinsic connection is lost with respect to the nature of the underlying data channels. For a human operator, a simplified system representation is only of value if it represents changing state quantities that have a meaning to the operator. It is the opinion of the author, after examining this analytical possibility, that a weighted basis of 112 different state variables is of questionable value with respect to operator intuition, so it was decided that this type of analysis would be excluded as a visualization target.

\subsection{Ad Hoc Dimensionality Reduction Techniques}

In addition to identifying principal components of the system and displaying only those modes with significant singular values, we also can look for system-specific optimizations to reduce dimensionality.

The simplest such technique is to remove data channels that are not of interest. Channels that always behave deterministically, such as known functions of other channels or channels that stay constant throughout an entire telemetry recording, are prime candidates for removal. (The latter set of channels could not be covered in correlative analysis anyway, because the  standard deviation for this set would be zero.)

Another candidate for dimensionality reduction is the scope of correlative calculation. Instead of correlating all $n$ channels with each other, if channels are divided into independent subsystem clusters, then the channels within these subsystem clusters may be correlated internally, without inter-correlations among the clusters. However, this eliminates the possibility of discovering unintended data connections between subsystems, which could be indicative of unanticipated and important fault-related behavior.

Both of these approaches are highly supervised and require expert knowledge of the system, constraining their general applicability and restricting their capacity to discover unintended patterns in the data. However, they could prove useful in the absence of better techniques.

% Tharrault paper

% Russell paper

% \section{Meta-Analysis}

% \subsection{Out-of-Family Telemetry}

% Two NASA papers

% \subsection{Out-of-Family Correlations}

% \section{Corrgram Enhancements and Dimensional Reduction}

% \subsection{Smoothing and Time Adjustments}

% \subsection{Ranked Filtering}

% \subsection{Fault Filtering}

% \subsection{Substring Filtering}

% \subsection{Cross-System Filtering}

% \subsection{Timelines}

\section{Two-Dimensional Graph Embeddings}

Since the vast majority of user interfaces in common usage are two-dimensional, and hardware limitations can easily result in 3D user interfaces being infeasible for users, it makes sense to look at ways that $n$-dimensional state data can be embedded within a 2D visualization. Towards this purpose, we did a brief survey of state-of-the-art 2D graph embeddings, looking for implementation feasibility and the ability to give a user ``insight" into the nature of a system fault. Preferably, a 2D embedding for system understanding will make major state transitions and patterns visibly obvious at a glance, and will spatially separate different system ``modes" so that they can easily be mentally grouped by the viewer.

\subsection{Undirected Correlation Graphs}

The first type of two-dimensional graph embedding that we examined as an alternative for animated corrgrams was the ``dependency graph." Dependency graphs are a type of 2D embedding in which a complex system is represented as a directed graph, where nodes are system components and edges represent dependencies; for example, if $\textbf{node}_{A} \rightarrow \textbf{node}_{B}$ and $\textbf{node}_{B} \rightarrow \textbf{node}_{C}$, this indicates that the value of $\textbf{node}_{A}$ depends on the value of $\textbf{node}_{B}$, which in turn depends on $\textbf{node}_{C}$. A simple example of this type of visualization is shown in Fig.~\ref{fig:dependency_graph_example}.

\begin{figure}[h]
\centering
    \includegraphics{images/dependency_graph_example.png}
    \caption{A simple example of a traditional dependency graph is shown. Here, node A's value depends on the value of node B, and node B's value depends on the value of node C.}
    \label{fig:dependency_graph_example}
\end{figure}

This type of visualization lends itself well to illustrating the effect of causation in a system. Though causation can be difficult to determine definitively, as we have already shown, correlation is calculable via the PCC and other metrics, and a correlation-generalized ``undirected correlation graph" could be envisioned, wherein two nodes have an undirected edge if their mutual correlation score exceeds a certain threshold, and have no edge if their mutual correlation score fails to meet that threshold. The steps to generate such a graph are as follows:

\begin{enumerate}
    \item At a given point in time, generate a correlation matrix for all of the possible data channel pairs.
    \item Generate an unconnected graph in which there exists a degree-0 node for each data channel.
    \item For each node-node pair, add a connecting edge if they have a mutual correlation score above a reasonably high correlation threshold (e.g., $r_{PCC}^{2} > 0.8$). This edge can be colored to show the sign of the correlation (e.g., blue for $r_{PCC} < 0$ and red for $r_{PCC} > 0$).
    \item Finally, cull all nodes that are still of degree 0.
\end{enumerate}

With the corrgram visualization, we needed to illustrate every possible pair of channels as a separate cell, and thus ended up needing to visualize $\frac{n!}{2 (n - 2)!}$ different cells (where $n$ is the number of data channels). However, with a dependency graph visualization, we can reduce the number of colored elements (nodes) to a count of $n$, by introducing connecting edges. For systems that are not highly correlated, this will produce far less visual clutter than the corrgram visualization. Furthermore, we greatly simplify the undirected dependency graph visualization by eliminating any components of degree 0, as the correlated components are the only ones that we wish to see.

We experimented with actually creating this visualization for explorational purposes. First, we ran a system dynamics simulation for our lunar rover, as described in Chapter 5. We paused the telemetry analysis at a time step at which interesting correlated components were present, and examined the data channel correlations at that instant. The correlated components are illustrated in the ``correlation map" animated corrgram visualization in Fig.~\ref{fig:comparison_correlation_map}. We then isolated the correlated components and illustrated them as an undirected dependency graph, using the steps outlined above. The resulting visualization, with positive and negative correlation edges both visible, is shown in Fig.~\ref{fig:undirected_both}. In addition, positive and negative correlated components have been isolated into separate graphs for readability and discoverability, as shown in Fig.~\ref{fig:undirected_positive_only} and Fig.~\ref{fig:undirected_negative_only}, respectively.

\begin{figure}[h]
\centering
    \includegraphics[width=\columnwidth]{images/comparison_correlation_map.png}
    \caption{A snapshot of the correlation map display from a simulated run. Note the strong correlations illustrated by opaque orange (positive) and blue (negative) cells.}
    \label{fig:comparison_correlation_map}
\end{figure}

\begin{figure}[h]
\centering
    \includegraphics[width=\columnwidth]{images/undirected_both.png}
    \caption{A snapshot of an undirected dependency graph display from a simulated run. Correlated components have been isolated, with edges drawn for all correlation relationships exceeding a certain value ($r_{PCC}^{2} > 0.8$). Both positive and negative correlation connections are shown. Self-correlations are not shown.}
    \label{fig:undirected_both}
\end{figure}

\begin{figure}[h]
\centering
    \includegraphics[width=\columnwidth]{images/undirected_positive_only.png}
    \caption{A snapshot of an undirected dependency graph display from a simulated run. Correlated components have been isolated, with edges drawn for all correlation relationships exceeding a certain value ($r_{PCC}^{2} > 0.8$). Only positive correlations are shown. Self-correlations are not shown.}
    \label{fig:undirected_positive_only}
\end{figure}

\begin{figure}[h]
\centering
    \includegraphics[width=\columnwidth]{images/undirected_negative_only.png}
    \caption{A snapshot of an undirected dependency graph display from a simulated run. Correlated components have been isolated, with edges drawn for all correlation relationships exceeding a certain value ($r_{PCC}^{2} > 0.8$). Only negative correlations are shown. Self-correlations are not shown.}
    \label{fig:undirected_negative_only}
\end{figure}

These visualizations present a very different way of viewing the correlative data. While the temporal dimension is still only captured as a snapshot (i.e., the correlative state at only one time point can be shown at a time), correlated components are shown very clearly, and can be understood at a glance. In particular, the intuition behind the positive correlated components in Fig.~\ref{fig:undirected_positive_only} seems clear; the most fully-connected, major clusters are exhibiting behavior which is very similar to each other. (In fact, the lower left cluster channels were all in a state of monotonic decrease, and the upper right cluster channels were in a state of monotonic increase.) The negative correlated components are less obvious, as they don't ``cluster" in the same way; however, the negative correlation data can be overlaid onto the positive correlation graph as a higher-level operation on the clusters. This, perhaps, produces the most informative, while simultaneously intuitive, type of graph; this application is shown in Fig.~\ref{fig:undirected_positive_with_negative_clusters}.

Note that \cite{yeh2007exploratory} uses a similar approach for graph visualization of correlation matrix relationships; however, they impose a radial structure on all graph layouts, which loses the clustering advantage of the graph layouts we have produced here.

\begin{figure}[h]
\centering
    \includegraphics[width=\columnwidth]{images/undirected_positive_with_negative_clusters.png}
    \caption{A snapshot of an undirected dependency graph display from a simulated run. Correlated components have been isolated, with edges drawn for all correlation relationships exceeding a certain value ($r_{PCC}^{2} > 0.8$). Positive correlations, and negatively correlated subgraphs, are shown. Self-correlations are not shown.}
    \label{fig:undirected_positive_with_negative_clusters}
\end{figure}

Another advantage of this technique is that it clearly isolates small groups of correlated components; low-degree subgraphs can point towards noisy data, if they are transient; however, if they are persistent, they may suggest interesting correlations that deviate from the patterns exhibited by the bulk of the data channels (which tend to correlate due to behavior that exhibits positive or negative monotonicity).

This visualization is extremely useful for capturing correlation state at a point in time. In the next section, we will look at a technique that is capable of showing the evolution of state and correlative data over time.

\section{Time Curves}

In early 2016, Bach et al. presented a powerful new type of visualization tool called ``Time Curves" \cite{bach2016time}. The time curve is a generic 2D embedding algorithm designed specifically for system state data which changes over time. Time curves visualize system states as a series of points, connected along temporal curves within a 2D embedding. This allows the viewer to gain an understanding of system behavior by the shape and directions of the curves, and by the grouping of the data points. A visual example of how a time curve is ``folded" from an initial linear timeline is shown in Fig.~\ref{fig:time_curve_example}.

\begin{figure}[h]
\centering
    \includegraphics[width=0.5\columnwidth]{images/time_curve_example.png}
    \caption{A time curve is ``folded" from an initial linear timeline to bring similar data points close together in the 2D embedding. From \cite{bach2016time}.}
    \label{fig:time_curve_example}
\end{figure}

Let's take a closer look at how time curves model system behavior. In the time curves description, states over time are described as a series of ``time points," where each time point is a vector $\bar{x} \in \mathbb{R}^{n}$. For $t$ time points, the state at each time point can be horizontally packed into a matrix $M \in \mathbb{R}^{n \times t}$, which represents the full state history of the system.

The time curves algorithm uses a handful of multidimensional scaling (MDS) techniques which seek to map a user-supplied measure of state vector distance to 2D Euclidean embedding distance, such that significantly ``similar" states are nearby in the embedding, while significantly ``different" states are far away in the embedding. The essential component to generating this embedding is the construction of a symmetric distance matrix $D \in \mathbb{R}^{t \times t}$, such that $D_{ij} = D_{ji}$ provides a pairwise distance score representing the difference between the states $M_{:,i}$ and $M_{:,j}$. A typical choice for this distance is a Euclidean norm \cite{bach2016time}, though many alternatives exist, such as the cosine distance or Hamming distance. The time curves algorithm performs an MDS optimization step to map the pairwise state distances described in $D$ to a 2D graph embedding, producing the final visualization.

The time curves algorithm has many characteristics which make it an appealing candidate for complex system visualization. It exhibits \textbf{clustering}, where time points of similar states gather at very close points in the embedding. The viewer's eye naturally processes these clusters as single groups, and they may easily map to system modes. Time curves visualize \textbf{cycles}, where a time curve comes back to an earlier point in the embedding after moving elsewhere. This is very useful for modeling cyclic dynamics in complex systems. Time curves also emphasize \textbf{transitions} between clusters of points, as the embedding naturally pushes distinct clusters geometrically apart from each other.

Finally, time curves are very extensible, and can be combined with higher-level data which provides further insight into the state at a given time point.

\section{Applying Time Curves to System Telemetry Data}

We set about applying time curves to complex system telemetry data, in order to compare visualization results for raw state telemetry and correlative states, and to see if anomalous system modes could easily be identified in the resulting embeddings. Our data processing pipeline for raw state telemetry visualization was as follows:

\begin{enumerate}
    \item At regular intervals (e.g., once per second), record the telemetered system state into a vector $\bar{x} \in \mathbb{R}^{n}$.
    \item Once the mission is complete, horizontally concatenate all state vectors to produce a state history matrix $M \in \mathbb{R}^{n \times t}$.
    \item Generate a symmetric distance matrix $D \in \mathbb{R}^{t \times t}$, such that $D_{ij} = D_{ji}$ provides a pairwise distance score representing the difference between the states $M_{:,i}$ and $M_{:,j}$.
    \item Use the time curves algorithm to generate a 2D embedding of the $t$ states described in $D$.
\end{enumerate}

In contrast, our pipeline for generating visualizations for correlated telemetry was as follows:

\begin{enumerate}
    \item Generate a state history matrix $M$ as with the raw state telemetry case.
    \item For a given window size $w$, generate $t - w + 1$ windowed ``sample matrices" $W \in \mathbb{R}^{n \times w}$, such that $W_{i} = M_{:, i:i+w-1}$.
    \item For each sample matrix $W$, generate a correlative matrix $C \in \mathbb{R}^{n \times n}$ (via PCC, Spearman's Rho, or Kendall's Tau), such that $C_{ij} = C_{ji}$ gives the correlation score of the two data channels $W_{i,:}$ and $W_{j,:}$. (Note that incalculable correlations, such as those with data channels of zero standard deviation, are set to be 0.)
    \item Flatten each correlative matrix $C$, and concatenate the resulting column vectors horizontally into a matrix $C_{all} \in \mathbb{R}^{n^{2} \times (t - w + 1)}$, such that each column of $C_{all}$ represents the total correlative state of the system at a time point.
    \item Generate a symmetric distance matrix $D \in \mathbb{R}^{(t - w + 1) \times (t - w + 1)}$, such that $D_{ij} = D_{ji}$ provides a pairwise distance score representing the difference between the states $C_{:,i}$ and $C_{:,j}$.
    \item Use the time curves algorithm to generate a 2D embedding of the $(t - w + 1)$ states described in $D$.
\end{enumerate}

The algorithms described above were applied on the usability testing data set used previously during user evaluations. This data set closely simulates telemetry from a lunar mission, in which the events described in Tbl.~\ref{tbl:events} take place at predetermined times. Each of these events triggers numerous differences in simulated environment state, which in turn causes major sensor data differences. High levels of Gaussian white noise have been introduced into the sensor data to approximate real sensor behavior.

The annotated time curve visualization of the raw state telemetry progression is shown in Fig.~\ref{fig:pfm2_raw_data_time_curve_annotated}. Similarly annotated visualizations of the PCC, Spearman's Rho, and Kendall's Tau correlative states are shown in Figs.~\ref{fig:pfm2_pcc_time_curve_annotated}, \ref{fig:pfm2_rho_time_curve_annotated}, and \ref{fig:pfm2_tau_time_curve_annotated}, respectively. (These correlative visualizations were generated with a 15-second time window, with telemetry updates coming at the approximate rate of once per second.)

\section{Discussion}

The time curves visualization does a pleasing job of showing the varying modes of system operation, and allows the viewer to follow state progression easily. The major moves between groups of points correspond nearly exactly to the times during the simulation when the rover started experiencing telemetry differences from changed environmental conditions.

It is also evident, by inspection, that the state progression from correlative analysis in Figs.~\ref{fig:pfm2_pcc_time_curve_annotated}, \ref{fig:pfm2_rho_time_curve_annotated}, and \ref{fig:pfm2_tau_time_curve_annotated} is more distinct and easy to understand than that visualized using raw telemetry in Fig.~\ref{fig:pfm2_raw_data_time_curve_annotated}. We believe that there are several reasons for these core differences. First, monotonically increasing data channels (e.g., time or wheel rotations) can make it hard to isolate modes, as state vectors containing this data appear to represent different states at each time step, even when they really only represent a single mode of behavior. These data channels can ``smear" the state across a 2D embedding, due to a steady, constant distance introduced through these increasing channels. Unless they are deliberately removed from the data--which could, as a side-effect, remove valuable data crucial to understanding system behavior--these data channels will result in side-effects like those seen in Fig.~\ref{fig:pfm2_pcc_time_curve_annotated}, where the initial state and final state are actually very similar system modes, but are distant within the 2D embedding.

Additionally, environmental dynamics, and thus, the inputs into the system, can change as the environment changes, but if the vehicle subsystems continue to respond to these dynamics in the same way as before, then the correlative relationships among its data channels may maintain more consistency in spite of the changing dynamics. Correlative analysis, in this way, allows a human operator to more directly investigate internal system relationships (although external dynamics still manifest themselves in the correlative data as correlations of sensor values with time and other monotonically increasing values).

Finally, it appears that the correlative calculations, as they act on a large time window, appear to have had a denoising side-effect on the data, and allow real changes in correlation to clearly ``push" the time points around the embedding, making state transitions more clear--while state transitions within the raw telemetry visualization are subtle and mostly buried in the data's noise.

Another interesting observation about the data produced, especially visible within the correlative time curve visualizations, is the presence of additional states not foreseen in the event timeline, such as the cluster of time points beginning at point \textbf{2}. The system mode ``discovered" by the time curves visualization here corresponds exactly to when the simulated radio signal gets low enough to start simulating packet drops; however, this was not a planned event in the original timeline, and the raw telemetry does not change dramatically at this point.  The time curve is primarily illustrating a side-effect of the absence of data, rather than a stark change in the content of the data. The side-effect is primarily due to timing changes from certain data channels (whose packets are being dropped) updating less frequently, but manifests itself as a different correlative state. This effect is particularly interesting because it shows that our technique discovered a mode that a) we didn't anticipate, but which stands up against scrutiny, and b) is impossible to find in the same way with raw telemetry analysis.

\begin{table}[]
\centering
\begin{tabular}{lll}
Event Number & Timestamp & Event Description \\
0 & 0s & Rover begins traveling forward along smooth terrain. \\
1 & 188s & Rover enters crater; begins descending into crater. \\
2 & 223s & Rover loses line of sight with lander and packet drops begin. \\
3 & 287s & Rover enters shade, causing temp, comms, and power drops. \\
4 & 300s & Rover begins traversing smooth bottom of crater. \\
5 & 330s & Rover begins climbing out of crater. \\
6 & 343s & Rover exits shade; continues uphill. \\
7 & 534s & Rover emerges from crater and enters smooth terrain. \\
8 & 594s & Rover enters choppy terrain. \\
9 & 643s & Rover wheel has fault; rover stops moving.
\end{tabular}
\caption{Events during a visualized user simulation are shown. Cross-reference ``Event Numbers" with labels in Figs.~\ref{fig:pfm2_pcc_time_curve_annotated}, \ref{fig:pfm2_tau_time_curve_annotated}, \ref{fig:pfm2_rho_time_curve_annotated} and \ref{fig:pfm2_raw_data_time_curve_annotated} to see correspondence.}
\label{tbl:events}
\end{table}

\begin{figure}[h]
\centering
    \includegraphics{images/pfm2_raw_data_time_curve_annotated.png}
    \caption{A time curve embedding visualizing mission events using raw telemetry state. Event annotations are described in Tbl.~\ref{tbl:events}.}
    \label{fig:pfm2_raw_data_time_curve_annotated}
\end{figure}

\begin{figure}[h]
\centering
    \includegraphics{images/pfm2_pcc_time_curve_annotated.png}
    \caption{A time curve embedding visualizing mission events using PCC correlation state. Event annotations are described in Tbl.~\ref{tbl:events}.}
    \label{fig:pfm2_pcc_time_curve_annotated}
\end{figure}

\begin{figure}[h]
\centering
    \includegraphics{images/pfm2_rho_time_curve_annotated.png}
    \caption{A time curve embedding visualizing mission events using Spearman's Rho correlation state. Event annotations are described in Tbl.~\ref{tbl:events}.}
    \label{fig:pfm2_rho_time_curve_annotated}
\end{figure}

\begin{figure}[h]
\centering
    \includegraphics{images/pfm2_tau_time_curve_annotated.png}
    \caption{A time curve embedding visualizing mission events using Kendall's Tau correlation state. Event annotations are described in Tbl.~\ref{tbl:events}.}
    \label{fig:pfm2_tau_time_curve_annotated}
\end{figure}

It is the opinion of the author that the time curve visualization of correlative state data is extremely useful. Not only does it clearly separate and show system modes that we can validate against human intuition, but it allows a human operator to readily spot deviation from a given correlative state trajectory, in a way that seems more robust and reliable than raw state visualization. Future work to adapt this tool to the task of root cause analysis for complex systems will be described in the next chapter.